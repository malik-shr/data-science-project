---
title: "Coursework - Data Science I"
author: "Malik Sharkawy, 222234988"
output:
  html_notebook:
    fig_width: 10
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document: default
  html_document:
    fig_width: 10
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{=html}
<script>
$(document).ready(function() {
  $items = $('div#TOC li');
  $items.each(function(idx) {
    num_ul = $(this).parentsUntil('#TOC').length;
    $(this).css({'text-indent': num_ul * 10, 'padding-left': 0});
  });

});
</script>
```

```{r setup, warning=FALSE, message=FALSE, echo=FALSE}
library(svglite)
library(knitr)
suppressPackageStartupMessages(library(data.table))
library(ggplot2)
knitr::opts_chunk$set(dev = "svglite")

# Put your dataset in the same folder as your R file. This code will set your working directory for this notebook to the folder where the R file is stored. This way I can rerun your code without modifications.

library(rstudioapi)
setwd(dirname(getActiveDocumentContext()$path))
```

# Introduction

# Collection / Preparation

First of all we will import the libraries, that we need for our project.

```{r}
library(data.table)
library(ggplot2)
library(plot3D)
library(viridisLite)
library(FNN)
library(rpart)
library(rpart.plot)
library(viridisLite)
library(ggplot2)
```

Lets read the input of our dataset

```{r}
dt_happiness <- fread(file = "Datasets/WHR_2024.csv", sep = ",")
```

First of all I will explore the basic structure of the dataset

```{r}
str(dt_happiness)
```

First of all we convert all empty strings to Na, so our omit function can remove rows, where attributes are empty string

```{r}
dt_happiness[  
  dt_happiness$country == "" 
  | dt_happiness$region == ""
  | dt_happiness$happiness_score == 0 
  | dt_happiness$gdp_per_capita == 0 
  | dt_happiness$social_support == 0
  | dt_happiness$healthy_life_expectancy == 0
  | dt_happiness$freedom_to_make_life_choices == 0
  | dt_happiness$generosity == 0
  | dt_happiness$perceptions_of_corruption == 0
] <- NA

dt_happiness <- na.omit(dt_happiness)
```

Lets see the first 6 rows of our dataset.

```{r}
head(dt_happiness) # First 6 lines
```

Now I will define some reusable functions:

```{r}
calculate_tree_metrics <- function(model, dataset, key) {
  y_value <- dataset[[key]]
  y_pred <- predict(model, dataset)
  
  rmse <- sqrt(mean((y_value - y_pred)^2))
  r2 <- 1 - sum((y_value - y_pred)^2) / sum((y_value - mean(y_value))^2)
  
  list(
    rmse = rmse,
    r2 = r2
  )
}

calculate_lm_metrics <- function(model) {
  rmse <- sqrt(mean(model$residuals^2))
  r2 <- summary(model)$r.squared
  
  list(
    rmse = rmse,
    r2 = r2
  )
}

```

# Exploration
Lets summarize our data

```{r}
summary(dt_happiness)
```

Now we will have a look at the density of the happiness score.

```{r}
ggplot(dt_happiness, aes(x = happiness_score)) +
  geom_density(fill = "steelblue", alpha = 0.5) +
  geom_vline(xintercept = mean(dt_happiness$happiness_score), linetype="dashed")
```

We can see that the happiness_score of most countries is around 5.7 and 6.5. The vertical dashed-line represents, the mean happiness_score. It is at around 5.5.

The next plot will visualize the distribution of the happiness score depending on the GDP per capita

```{r}
ggplot(dt_happiness, aes(gdp_per_capita, happiness_score)) +
  geom_point(alpha = 0.5) 
```

We can see, that with a higher gdp the happinnes_score is higher on average. We can clearly say a low gdp_per_capita results in a low happiness_score and a high gdp_per_capita results into a high happiness_score. Nevertheless there is a spread at almost every gdp level. That tells us that the gdp can explain some but not all of the happiness.

In the next step I will visualize the gdp distribution for each region.

```{r}
ggplot(dt_happiness, aes(region, gdp_per_capita)) +
  geom_boxplot() +
  coord_flip() 
```

We can clearly see that a region occupies a distinct GDP range. The GDP is not distributed randomly.

Next I want to see the distribution of the happiness score for each region.

```{r}
ggplot(dt_happiness, aes(region, happiness_score)) +
  geom_boxplot() +
  coord_flip() 
```

We can see major differences between regions, when measuring their happiness score. Western Europe and North America and ANZ achieving the highest score, while Sub-Saharan Africa and South Asia achieving the lowest score. We have smaller boxes such as in North America and ANZ which indicates, that the countries based in the region, have a similar happiness index. Regions with wider boxes, such as countries in the Middle East and North Africa, have a wider distributed happiness score.

# Models 1 & 2

## Linear model

Lets begin to make our model. Our dependend variable is the happiness_score and our input variables are: the gdp_per_capita, the social_support, the healthy_life_expectancy, the freedom_to_make_life_choices, the generosity and the perceptions_of_corruption

```{r}
lm <- lm(
  happiness_score ~ 
  +gdp_per_capita 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

```{r}
lm$coefficients
```

```{r}
summary(lm)
```

```{r}
lm_metrics <- calculate_lm_metrics(lm)

lm_metrics$rmse
lm_metrics$r2
```

That means that the linear regression model explains around 80.5% percent of the variance in the response of the variable.

## Tree model

```{r}
tree <- rpart(
  happiness_score ~ 
  gdp_per_capita 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree, digits = -3)
```

```{r}
printcp(tree)
```

```{r}
tree
```

```{r}
plotcp(tree)
```

```{r}
tree_metrics <- calculate_tree_metrics(tree, dt_happiness, "happiness_score")

tree_metrics$rmse
tree_metrics$r2
```

That means that the tree regression model explains around 84% percent of the variance in the response of the variable.

## Comparing

Lets compare the rmse values of the tree and the linear model

```{r}
((lm_metrics$rmse - tree_metrics$rmse)/lm_metrics$rmse) * 100
```

The regression tree achieves an approximately 9.35% lower RMSE than the linear model, indicating a improvement in predictive accuracy.

```{r}
(tree_metrics$r2 - lm_metrics$r2) * 100
```

The regression tree explains 3.5 additional percentage points of variance compared to the linear model

# Feature Engineering

## Feature 1

Lets add the feature as a new column to our table

```{r}
dt_happiness$log_gdp <- log(dt_happiness$gdp_per_capita)
```

### Linear Model

We will make the gdp as a logarithmic variable

```{r}
lm1 <- lm(
  happiness_score ~ 
  log_gdp
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

Calculate r2 and rmse of our new model

```{r}
lm1_metrics <- calculate_lm_metrics(lm1)
```


### Tree Model
```{r}
tree1 <- rpart(
  happiness_score ~ 
  log_gdp 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree1, digits = -3)
```

```{r}
tree1
```

```{r}
plotcp(tree1)
```

```{r}
tree1_metrics <- calculate_tree_metrics(tree1, dt_happiness, "happiness_score")
```

## Feature 2

Lets add the feature as a new column to our table

```{r}
dt_happiness$gdp_x_support <- dt_happiness$gdp_per_capita * dt_happiness$social_support
```

### Linear Model

We will make the gdp as a logarithmic variable

```{r}
lm2 <- lm(
  happiness_score ~ 
  gdp_x_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

Calculate r2 and rmse of our new model

```{r}
lm2_metrics <- calculate_lm_metrics(lm2)
```

### Tree Model

```{r}
tree2 <- rpart(
  happiness_score ~ 
  gdp_x_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree2, digits = -3)
```

```{r}
tree2
```

```{r}
plotcp(tree2)
```

```{r}
tree2_metrics <- calculate_tree_metrics(tree2, dt_happiness, "happiness_score")
```

# Comparison

```{r}
dt_feature <- data.table(
  Model = c(
    "Linear Regression",
    "Linear Regression (Feature 1)",
    "Linear Model (Feature 2)",
    "Tree Regression",
    "Tree Regression (Feature 1)",
    "Tree Regression (Feature 2)"
  ),
  RMSE = c(
    lm_metrics$rmse,
    lm1_metrics$rmse,
    lm2_metrics$rmse,
    tree_metrics$rmse,
    tree1_metrics$rmse,
    tree2_metrics$rmse
  ),
  r2 = c(
    lm_metrics$r2,
    lm1_metrics$r2,
    lm2_metrics$r2,
    tree_metrics$r2,
    tree1_metrics$r2,
    tree2_metrics$r2
  )
)

setorder(dt_feature, RMSE, -r2)



dt_feature[,RMSE_rank := frank(RMSE, ties.method = "min")][,r2_rank := frank(-r2, ties.method = "min")]

head(dt_feature)
```

We can see that the Tree regression model without any features and the tree regression model with Feature 1 are performing the best.

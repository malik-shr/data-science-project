---
title: "Coursework - Data Science I"
author: "Malik Sharkawy, 222234988"
output:
  html_notebook:
    fig_width: 10
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document: default
  html_document:
    fig_width: 10
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{=html}
<script>
$(document).ready(function() {
  $items = $('div#TOC li');
  $items.each(function(idx) {
    num_ul = $(this).parentsUntil('#TOC').length;
    $(this).css({'text-indent': num_ul * 10, 'padding-left': 0});
  });

});
</script>
```

```{r setup, warning=FALSE, message=FALSE, echo=FALSE}
library(svglite)
library(knitr)
suppressPackageStartupMessages(library(data.table))
library(ggplot2)
knitr::opts_chunk$set(dev = "svglite")

# Put your dataset in the same folder as your R file. This code will set your working directory for this notebook to the folder where the R file is stored. This way I can rerun your code without modifications.

library(rstudioapi)
setwd(dirname(getActiveDocumentContext()$path))
```

# Introduction

In this project, we analyze how a country’s happiness score is related to factors such as social support, healthy life expectancy, freedom of choice, generosity, and perceptions of corruption. To do this, we use both linear and tree-based regression models and compare how well they predict happiness.

The dependent variable is the happiness score, while the other variables are independent. Since the happiness score is a continuous numerical value, this is a regression problem.

Social support describes how supported people feel by friends and family. Healthy life expectancy measures how many years people are expected to live in good health. Freedom of choice reflects how much control people feel they have over their lives. Generosity indicates how willing people are to help others, and perceptions of corruption show how people view corruption in public institutions.

All variables are based on standardized data, making it possible to compare different countries.

# Collection / Preparation

First of all we will import all libraries, that we need for our project.

```{r}
library(data.table)
library(ggplot2)
library(plot3D)
library(viridisLite)
library(FNN)
library(rpart)
library(rpart.plot)
library(viridisLite)
library(ggplot2)
```

Let's read the input of the dataset.

```{r}
dt_happiness <- fread(file = "Datasets/WHR_2024.csv", sep = ",")
```

Let’s explore the structure of the dataset to determine how it should be cleaned.

```{r}
str(dt_happiness)
```

This code marks missing or invalid values as NA. Rows are set to NA if the country or region is empty, or if any important variable has the value 0. These values are treated as missing data.

Afterwards, na.omit() removes all rows that contain NA. This way, only complete and usable data remains in the dataset.

```{r}
dt_happiness[  
  dt_happiness$country == "" 
  | dt_happiness$region == ""
  | dt_happiness$happiness_score == 0 
  | dt_happiness$gdp_per_capita == 0 
  | dt_happiness$social_support == 0
  | dt_happiness$healthy_life_expectancy == 0
  | dt_happiness$freedom_to_make_life_choices == 0
  | dt_happiness$generosity == 0
  | dt_happiness$perceptions_of_corruption == 0
] <- NA

dt_happiness <- na.omit(dt_happiness)
```

Let’s take a look at the first six rows of our dataset.

```{r}
head(dt_happiness) # First 6 lines
```

Now I will define some reusable functions:

```{r}
calculate_tree_metrics <- function(model, dataset, key) {
  y_value <- dataset[[key]]
  y_pred <- predict(model, dataset)
  
  rmse <- sqrt(mean((y_value - y_pred)^2))
  r2 <- 1 - sum((y_value - y_pred)^2) / sum((y_value - mean(y_value))^2)
  
  list(
    rmse = rmse,
    r2 = r2
  )
}

calculate_lm_metrics <- function(model) {
  rmse <- sqrt(mean(model$residuals^2))
  r2 <- summary(model)$r.squared
  
  list(
    rmse = rmse,
    r2 = r2
  )
}

```

# Exploration
Lets summarize our data

```{r}
summary(dt_happiness)
```

Now we will have a look at the density of the happiness score.

```{r}
ggplot(dt_happiness, aes(x = happiness_score)) +
  geom_density(fill = "steelblue", alpha = 0.5) +
  geom_vline(xintercept = mean(dt_happiness$happiness_score), linetype="dashed")
```

We can see that the happiness scores of most countries are between 5.7 and 6.7. The vertical dashed line represents the mean happiness score, which is around 5.6.

Next, we look at how happiness is related to GDP per capita.

```{r}
ggplot(dt_happiness, aes(gdp_per_capita, happiness_score)) +
  geom_point(alpha = 0.5) 
```
We can see that countries with higher GDP per capita are usually happier. Low GDP is often linked to low happiness, and high GDP to higher happiness. However, there is still a lot of variation, which shows that GDP alone does not explain everything.


Now we examine the relationship between happiness and social support.
```{r}
ggplot(dt_happiness, aes(social_support, happiness_score)) +
  geom_point(alpha = 0.5) 
```
This plot looks very similar to the GDP plot. Countries with more social support tend to be happier.


Next, we look at freedom to make life choices.
```{r}
ggplot(dt_happiness, aes(freedom_to_make_life_choices, happiness_score)) +
  geom_point(alpha = 0.5) 
```
Here, we see a clear positive relationship. More freedom is usually linked to higher happiness, even though there is still some spread.


Now we look at generosity.
```{r}
ggplot(dt_happiness, aes(generosity, happiness_score)) +
  geom_point(alpha = 0.5) 
```
There is a lot of variation at all levels of generosity. This suggests that generosity alone does not strongly determine happiness.

Next, we examine perceptions of corruption.
```{r}
ggplot(dt_happiness, aes(perceptions_of_corruption, happiness_score)) +
  geom_point(alpha = 0.5) 
```
Up to a value of about 0.3, happiness scores vary a lot. After that, the spread becomes smaller. Interestingly, higher values seem to be linked to an above average happiness, which is unexpected.

Finally, we look at healthy life expectancy.
```{r}
ggplot(dt_happiness, aes(healthy_life_expectancy, happiness_score)) +
  geom_point(alpha = 0.5) 
```
We can see a clear positive trend. Countries where people live longer and healthier lives also tend to be happier, although there is still some variation.


In the next step I will visualize the GDP per capita distribution for each region.

```{r}
ggplot(dt_happiness, aes(region, gdp_per_capita)) +
  geom_boxplot() +
  coord_flip() 
```

We can clearly see that a region occupies a distinct GDP range. The GDP is not distributed randomly.

Next I want to see the distribution of the happiness score for each region.

```{r}
ggplot(dt_happiness, aes(region, happiness_score)) +
  geom_boxplot() +
  coord_flip() 
```

We can see major differences between regions, when measuring their happiness score. Western Europe and North America and ANZ achieving the highest score, while Sub-Saharan Africa and South Asia achieving the lowest score. We have smaller boxes such as in North America and ANZ which indicates, that the countries based in the region, have a similar happiness index. Regions with wider boxes, such as countries in the Middle East and North Africa, have a wider distributed happiness score.

# Models 1 & 2

## Linear model

Now, we begin building our model. The dependent variable is the happiness score, and the input variables are GDP per capita, social support, healthy life expectancy, freedom to make life choices, generosity, and perceptions of corruption.

```{r}
lm <- lm(
  happiness_score ~ 
  +gdp_per_capita 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

Next, we look at the model coefficients and summary.
```{r}
lm$coefficients
```

```{r}
summary(lm)
```

We also evaluate the performance of the model using RMSE and R².
```{r}
lm_metrics <- calculate_lm_metrics(lm)

lm_metrics$rmse
lm_metrics$r2
```

The results show that the linear regression model explains about 80.5% of the variance in the happiness score and has an average prediction error of around 0.49. This means that the model captures a large part of the relationship between happiness and the selected variables, although some variation remains unexplained.

## Tree model

Next, we build a regression tree to predict the happiness score using the same input variables.

```{r}
tree <- rpart(
  happiness_score ~ 
  gdp_per_capita 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree, digits = -3)
```
We then examine the complexity of the tree and its structure.
```{r}
printcp(tree)
```

```{r}
tree
```

```{r}
plotcp(tree)
```

Finally, we evaluate the model using RMSE and R².
```{r}
tree_metrics <- calculate_tree_metrics(tree, dt_happiness, "happiness_score")

tree_metrics$rmse
tree_metrics$r2
```

The results show that the tree-based regression model explains about 84% of the variance in the happiness score and has a typical prediction error (RMSE) of around 0.45. This indicates that the tree model performs slightly better than the linear regression model for this dataset.

## Comparing

Next, we compare the performance of the linear regression model and the regression tree using RMSE and R².

```{r}
((lm_metrics$rmse - tree_metrics$rmse)/lm_metrics$rmse) * 100
```
The regression tree achieves an approximately 9.35% lower RMSE than the linear model, indicating an improvement in predictive accuracy.

```{r}
(tree_metrics$r2 - lm_metrics$r2) * 100
```

In addition, the regression tree explains about 3.5 more percentage points of variance than the linear model.

Overall, these results suggest that the tree-based model performs slightly better than the linear model for this dataset, likely because it can capture non-linear relationships and interactions between the variables.

# Feature Engineering

To potentially improve the model performance, we create two additional features and test them in both the linear regression and the regression tree.

## Feature 1
First, we add the transformed variable as a new column:

```{r}
dt_happiness$log_gdp <- log(dt_happiness$gdp_per_capita)
```

### Linear Model

We fit a new linear regression model including both the original GDP per capita and the log-transformed GDP.

```{r}
lm1 <- lm(
  happiness_score ~
  gdp_per_capita
  +log_gdp
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

Then we evaluate the model using RMSE and R²:

```{r}
lm1_metrics <- calculate_lm_metrics(lm1)
```


### Tree Model
We repeat the same idea with a regression tree:
```{r}
tree1 <- rpart(
  happiness_score ~ 
  gdp_per_capita
  +log_gdp 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree1, digits = -3)
```

```{r}
tree1
```

```{r}
plotcp(tree1)
```

```{r}
tree1_metrics <- calculate_tree_metrics(tree1, dt_happiness, "happiness_score")
```

## Feature 2

We add the interaction term as a new column:

```{r}
dt_happiness$gdp_x_support <- dt_happiness$gdp_per_capita * dt_happiness$social_support
```

### Linear Model

We train a new linear model that includes the interaction feature:

```{r}
lm2 <- lm(
  happiness_score ~ 
  gdp_per_capita
  +gdp_x_support
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

And evaluate it:

```{r}
lm2_metrics <- calculate_lm_metrics(lm2)
```

### Tree Model
We also fit a regression tree using the same features:

```{r}
tree2 <- rpart(
  happiness_score ~ 
  gdp_per_capita
  +gdp_x_support
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree2, digits = -3)
```

```{r}
tree2
```

```{r}
plotcp(tree2)
```

```{r}
tree2_metrics <- calculate_tree_metrics(tree2, dt_happiness, "happiness_score")
```

# Comparison
To compare all models, we put RMSE and R² into one table and rank them. Lower RMSE means better predictions, while higher R² means the model explains more variance.

```{r}
dt_feature <- data.table(
  Model = c(
    "Linear Regression",
    "Linear Regression (Feature 1)",
    "Linear Model (Feature 2)",
    "Tree Regression",
    "Tree Regression (Feature 1)",
    "Tree Regression (Feature 2)"
  ),
  rmse = c(
    lm_metrics$rmse,
    lm1_metrics$rmse,
    lm2_metrics$rmse,
    tree_metrics$rmse,
    tree1_metrics$rmse,
    tree2_metrics$rmse
  ),
  r2 = c(
    lm_metrics$r2,
    lm1_metrics$r2,
    lm2_metrics$r2,
    tree_metrics$r2,
    tree1_metrics$r2,
    tree2_metrics$r2
  )
)

setorder(dt_feature, rmse, -r2)



dt_feature[,rmse_rank := frank(rmse, ties.method = "min")][,r2_rank := frank(-r2, ties.method = "min")]
 
head(dt_feature)
```
From the results, we can see that the tree regression model with Feature 2 performs best overall. It has the lowest RMSE and therefore gives the most accurate predictions among all tested models.

The tree regression model with Feature 2 performs best, likely because the interaction between GDP per capita and social support captures how economic wealth and social relationships work together to influence happiness. Instead of treating these factors separately, the model can now consider how their combined effect impacts well-being. This allows the tree model to better capture complex and non-linear relationships in the data, leading to more accurate predictions.

---
title: "Coursework - Data Science I"
author: "Malik Sharkawy, 222234988"
output:
  html_notebook:
    fig_width: 10
    theme: spacelab
    toc: yes
    toc_depth: 3
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document: default
  html_document:
    fig_width: 10
    theme: spacelab
    toc: yes
    toc_depth: 3s
    toc_float: yes
---

```{=html}
<script>
$(document).ready(function() {
  $items = $('div#TOC li');
  $items.each(function(idx) {
    num_ul = $(this).parentsUntil('#TOC').length;
    $(this).css({'text-indent': num_ul * 10, 'padding-left': 0});
  });

});
</script>
```

```{r setup, warning=FALSE, message=FALSE, echo=FALSE}
library(svglite)
library(knitr)
suppressPackageStartupMessages(library(data.table))
library(ggplot2)
knitr::opts_chunk$set(dev = "svglite")

# Put your dataset in the same folder as your R file. This code will set your working directory for this notebook to the folder where the R file is stored. This way I can rerun your code without modifications.

library(rstudioapi)
setwd(dirname(getActiveDocumentContext()$path))
```

# Introduction

In this project, we analyze how a country’s happiness score is related to factors such as social support, healthy life expectancy, freedom of choice, generosity, and perceptions of corruption. To do this, we use both linear and tree-based regression models and compare how well they predict happiness.

The dependent variable is the happiness score, while the other variables are independent. Since the happiness score is a continuous numerical value, this is a regression problem.

GDP per capita is a measure of Gross Domestic Product per capita, social support is a metric measuring social support, healthy life expectancy is a measure of years of healthy life expectancy, freedom to make life choices is a measure of freedom in life choices, generosity is a metric reflecting generosity, and perceptions of corruption is a measure of perception of corruption within a country.

# Collection / Preparation

First of all we will import all libraries, that we need for our project.

```{r}
library(data.table)
library(ggplot2)
library(plot3D)
library(viridisLite)
library(FNN)
library(rpart)
library(rpart.plot)
library(viridisLite)
library(ggplot2)
```

Let's read the input of the dataset.

```{r}
dt_happiness <- fread(file = "Datasets/WHR_2024.csv", sep = ",")
```

Let’s explore the structure of the dataset to determine how it should be cleaned.

```{r}
str(dt_happiness)
```

This code marks missing or invalid values as NA. Rows are set to NA if the country or region is empty, or if any important variable has the value 0. These values are treated as missing data.

Afterwards, na.omit() removes all rows that contain NA. This way, only complete and usable data remains in the dataset.

```{r}
dt_happiness[  
  dt_happiness$country == "" 
  | dt_happiness$region == ""
  | dt_happiness$happiness_score == 0 
  | dt_happiness$gdp_per_capita == 0 
  | dt_happiness$social_support == 0
  | dt_happiness$healthy_life_expectancy == 0
  | dt_happiness$freedom_to_make_life_choices == 0
  | dt_happiness$generosity == 0
  | dt_happiness$perceptions_of_corruption == 0
] <- NA

dt_happiness <- na.omit(dt_happiness)
```

Let’s take a look at the first six rows of our dataset.

```{r}
head(dt_happiness) # First 6 lines
```

Now I will define some reusable functions:

```{r}
calculate_tree_metrics <- function(model, dataset, key) {
  y_value <- dataset[[key]]
  y_pred <- predict(model, dataset)
  
  rmse <- sqrt(mean((y_value - y_pred)^2))
  r2 <- 1 - sum((y_value - y_pred)^2) / sum((y_value - mean(y_value))^2)
  
  list(
    rmse = rmse,
    r2 = r2
  )
}

calculate_lm_metrics <- function(model) {
  rmse <- sqrt(mean(model$residuals^2))
  r2 <- summary(model)$r.squared
  
  list(
    rmse = rmse,
    r2 = r2
  )
}

```

# Exploration
Lets summarize our data

```{r}
summary(dt_happiness)
```

Now we will have a look at the density of the happiness score.

```{r}
ggplot(dt_happiness, aes(x = happiness_score)) +
  geom_density(fill = "steelblue", alpha = 0.5) +
  geom_vline(xintercept = mean(dt_happiness$happiness_score), linetype="dashed")
```

We can see that the happiness scores of most countries are between 5.7 and 6.7. The vertical dashed line represents the mean happiness score, which is around 5.6.

Next, we look at how happiness is related to GDP per capita.

```{r}
ggplot(dt_happiness, aes(gdp_per_capita, happiness_score)) +
  geom_point(alpha = 0.5) 
```
We can see that countries with higher GDP per capita are usually happier. Low GDP is often linked to low happiness, and high GDP to higher happiness. However, there is still a lot of variation, which shows that GDP alone does not explain everything.


Now we examine the relationship between happiness and social support.
```{r}
ggplot(dt_happiness, aes(social_support, happiness_score)) +
  geom_point(alpha = 0.5) 
```
This plot looks very similar to the GDP plot. Countries with more social support tend to be happier.


Next, we look at freedom to make life choices.
```{r}
ggplot(dt_happiness, aes(freedom_to_make_life_choices, happiness_score)) +
  geom_point(alpha = 0.5) 
```
Here, we see a clear positive relationship. More freedom is usually linked to higher happiness, even though there is still some spread.


Now we look at generosity.
```{r}
ggplot(dt_happiness, aes(generosity, happiness_score)) +
  geom_point(alpha = 0.5) 
```
There is a lot of variation at all levels of generosity. This suggests that generosity alone does not strongly determine happiness.

Next, we examine perceptions of corruption.
```{r}
ggplot(dt_happiness, aes(perceptions_of_corruption, happiness_score)) +
  geom_point(alpha = 0.5) 
```
Up to a value of about 0.3, happiness scores vary a lot. After that, the spread becomes smaller. Interestingly, higher values seem to be linked to an above average happiness, which is unexpected.

Finally, we look at healthy life expectancy.
```{r}
ggplot(dt_happiness, aes(healthy_life_expectancy, happiness_score)) +
  geom_point(alpha = 0.5) 
```
We can see a clear positive trend. Countries where people live longer and healthier lives also tend to be happier, although there is still some variation.


In the next step I will visualize the GDP per capita distribution for each region.

```{r}
ggplot(dt_happiness, aes(region, gdp_per_capita)) +
  geom_boxplot() +
  coord_flip() 
```

We can clearly see that a region occupies a distinct GDP range. The GDP is not distributed randomly.

Next I want to see the distribution of the happiness score for each region.

```{r}
ggplot(dt_happiness, aes(region, happiness_score)) +
  geom_boxplot() +
  coord_flip() 
```

We can see major differences between regions, when measuring their happiness score. Western Europe and North America and ANZ achieving the highest score, while Sub-Saharan Africa and South Asia achieving the lowest score. We have smaller boxes such as in North America and ANZ which indicates, that the countries based in the region, have a similar happiness index. Regions with wider boxes, such as countries in the Middle East and North Africa, have a wider distributed happiness score.

# Models 1 & 2

## Linear model

Now, we begin building our model. The dependent variable is the happiness score, and the input variables are GDP per capita, social support, healthy life expectancy, freedom to make life choices, generosity, and perceptions of corruption.

```{r}
lm <- lm(
  happiness_score ~ 
  +gdp_per_capita 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

Next, we look at the model coefficients and summary.
```{r}
lm$coefficients
```

```{r}
summary(lm)
```

We also evaluate the performance of the model using RMSE and R².
```{r}
lm_metrics <- calculate_lm_metrics(lm)

lm_metrics$rmse
lm_metrics$r2
```

The results show that the linear regression model explains about 80.5% of the variance in the happiness score and has an average prediction error of around 0.49. This means that the model captures a large part of the relationship between happiness and the selected variables, although some variation remains unexplained.

## Tree model

Next, we build a regression tree to predict the happiness score using the same input variables.

```{r}
tree <- rpart(
  happiness_score ~ 
  gdp_per_capita 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree, digits = -3)
```
We then examine the complexity of the tree and its structure.
```{r}
printcp(tree)
```

```{r}
tree
```

```{r}
plotcp(tree)
```
The model selected social support, GDP per capita, freedom to make life choices, and generosity as the most relevant predictors, with social support being the primary splitting variable. Cross-validation results showed that prediction error decreased substantially with the first few splits and stabilized for larger trees.

Finally, we evaluate the model using RMSE and R².
```{r}
tree_metrics <- calculate_tree_metrics(tree, dt_happiness, "happiness_score")

tree_metrics$rmse
tree_metrics$r2
```

The results show that the tree-based regression model explains about 84% of the variance in the happiness score and has a typical prediction error (RMSE) of around 0.45. This indicates that the tree model performs slightly better than the linear regression model for this dataset.

## Comparing

Next, we compare the performance of the linear regression model and the regression tree using RMSE and R².

```{r}
((lm_metrics$rmse - tree_metrics$rmse)/lm_metrics$rmse) * 100
```
The regression tree achieves an approximately 9.35% lower RMSE than the linear model, indicating an improvement in predictive accuracy.

```{r}
(tree_metrics$r2 - lm_metrics$r2) * 100
```

In addition, the regression tree explains about 3.5 more percentage points of variance than the linear model.

Overall, these results suggest that the tree-based model performs slightly better than the linear model for this dataset, likely because it can capture non-linear relationships and interactions between the variables.

# Feature Engineering

To potentially improve the model performance, we create two additional features and test them in both the linear regression and the regression tree.

## Feature 1
First, we add the transformed variable as a new column:

```{r}
dt_happiness$log_gdp <- log(dt_happiness$gdp_per_capita)
```

### Linear Model

We fit a new linear regression model including both the original GDP per capita and the log-transformed GDP.

```{r}
lm1 <- lm(
  happiness_score ~
  gdp_per_capita
  +log_gdp
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

Then we evaluate the model using RMSE and R²:

```{r}
lm1_metrics <- calculate_lm_metrics(lm1)
```


### Tree Model
We repeat the same idea with a regression tree:
```{r}
tree1 <- rpart(
  happiness_score ~ 
  gdp_per_capita
  +log_gdp 
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree1, digits = -3)
```

```{r}
tree1
```

```{r}
plotcp(tree1)
```

```{r}
tree1_metrics <- calculate_tree_metrics(tree1, dt_happiness, "happiness_score")
```

## Feature 2

We add the interaction term as a new column:

```{r}
dt_happiness$gdp_x_support <- dt_happiness$gdp_per_capita * dt_happiness$social_support
```

### Linear Regression

We train a new linear model that includes the interaction feature:

```{r}
lm2 <- lm(
  happiness_score ~ 
  gdp_per_capita
  +gdp_x_support
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness
)
```

And evaluate it:

```{r}
lm2_metrics <- calculate_lm_metrics(lm2)
```

### Tree Regression
We also fit a regression tree using the same features:

```{r}
tree2 <- rpart(
  happiness_score ~ 
  gdp_per_capita
  +gdp_x_support
  +social_support
  +healthy_life_expectancy
  +freedom_to_make_life_choices
  +generosity 
  +perceptions_of_corruption,
  data = dt_happiness,
  method="anova"
)

prp(tree2, digits = -3)
```

```{r}
tree2
```

```{r}
plotcp(tree2)
```

```{r}
tree2_metrics <- calculate_tree_metrics(tree2, dt_happiness, "happiness_score")
```

# Comparison
To compare all models, we put RMSE and R² into one table and rank them. Lower RMSE means better predictions, while higher R² means the model explains more variance.

```{r}
dt_feature <- data.table(
  Model = c(
    "Linear Regression",
    "Linear Regression (Feature 1)",
    "Linear Model (Feature 2)",
    "Tree Regression",
    "Tree Regression (Feature 1)",
    "Tree Regression (Feature 2)"
  ),
  rmse = c(
    lm_metrics$rmse,
    lm1_metrics$rmse,
    lm2_metrics$rmse,
    tree_metrics$rmse,
    tree1_metrics$rmse,
    tree2_metrics$rmse
  ),
  r2 = c(
    lm_metrics$r2,
    lm1_metrics$r2,
    lm2_metrics$r2,
    tree_metrics$r2,
    tree1_metrics$r2,
    tree2_metrics$r2
  )
)

setorder(dt_feature, rmse, -r2)



dt_feature[,rmse_rank := frank(rmse, ties.method = "min")][,r2_rank := frank(-r2, ties.method = "min")]
 
head(dt_feature)
```

From the results, we can see that the tree regression model with Feature 2 performs best overall. It has the lowest RMSE and therefore gives the most accurate predictions among all tested models.

The regression tree model with Feature 2 performs best because it captures the interaction between GDP per capita and social support, allowing the combined influence of economic and social factors on happiness to be modeled. This enables the tree to represent non-linear relationships more accurately. The strong performance of the linear model also indicates that many relationships in the data are approximately linear.
